---
layout: post
title: "RL note #2"
date: 2017-12-31
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

#### 목표
- 강화학습의 기본 모델 학습

## 그리드 월드와 다이내믹 프로그래밍
### 그리드월드
- n * n의 격자 공간
- 이전에 가정했던 5 * 5 격자공간을 가정

### 동적 계획법(다이내믹 프로그래밍)
- 정의
  - 리처드 벨만에 의해서 만들어진 최적화 방정식
  - 나무위키 : https://namu.wiki/w/동적%20계획법
- 예제
  - https://www.acmicpc.net/problem/1003

### 동적 계획법을 통한 강화학습
- 가치(Q함수) update 시에 사용
-  V 	=  $E[𝑹_𝟏  + γ 𝑹_𝟐 + γ^𝟐 𝑹_𝟑   + … + γ^{𝒏−𝟏} 𝑹_𝒏]$<br>
  	=   $E[𝑹_𝟏 + γV’]$
-  q*(s)  	= E[R + γ max q* (𝑺_(𝒕+𝟏), a’) | 𝑺_𝒕 = s, 𝑨_𝒕=𝒂 ]
- 벨만 기대 방정식 -> 정책 이터레이션
- 벨만 최적 방정식 -> 가치 이터레이션

### 정책 평가와 정책 발전
- 정책평가 = Q함수 값들을 이용해 현재 노드 가치 update
- 정책발전(탐욕정책) = Q함수값들을 이용해 현재 노드의 정책 update

### 정책 이터레이션
- 평가 -> 평가 -> 발전 -> 평가 -> ... -> 평가 -> **행동**
- 평가시 이전 정책을 토대로 평가

### 가치 이터레이션
- 정책 발전이 존재하지 않음
- 평가 -> 평가 ->  ... -> 평가 -> **행동**
-  평가시 정책과 상관없이 최대Q함수로 가치 update

Q : 정책 이터레이션도 탐욕정책을 취하기에 가장 큰 Q함수를 가진 방향의 정책만을 취하게 되는데 그러면 가치 이터레이션과 동일하다?<br>
A : 정책 이터레이션은 정책이 명확히 명시되어 있어 그 정책에 따라서만 발전한다.<br>
하지만 가치 이터레이션은 정책이 명확히 명시되어 있지 않고 최대 Q함수로만 발전된다.
{: .notice}


<hr/>
### 문제점
- 환경에 대한 정보부족 문제
- 계산복잡도가 높다는 문제

<hr/>
