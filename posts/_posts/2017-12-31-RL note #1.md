---
layout: post
title: "RL note #1"
date: 2017-12-31
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

---

## 강화학습이란
### 개요
- 시행착오를 통하여 학습하는 알고리즘
- 실제상황
  - 상황이 주어짐 (자전거를 타는 상황)
  - 에이전트가 행동 (오른쪽으로 핸들 돌리기)
  - 환경이 보상 (넘어짐 / 안넘어짐)

### 적용가능한 문제들
-  경로 찾기, 스케줄링
-  미로찾기, Frozen Lake, TSP …

---

## 기본 모델
### 순차적 행동 결정 문제
-  순차적으로 행동을 결정해야하는 문제들
-  5 * 5 격자공간에서 이동 최단거리를 탐색 문제로 가정
( (0, 0)에서 시작, (4, 4)에서 종료 )

### 순차적 행동 결정 문제의 구성 요소
- 상태(state) : </tab>현재 내가 어디에 있는지
- 행동(action) : 어디로 이동할지
- 보상(reward) : 행동 했을 때 좋은 행동인지 안 좋은 행동인지
- 정책(policy) : 각각의 상황에서 어디로 이동할 지에 대한 표지판

---

## MDP와 벨만 방정식
### MDP(Markov Decision Process)
- 순차적 행동결정 문제를 수학적 정의하여 프로세스를 설명
- 상태 / 행동 / 보상 / 상태 변환 확률 / 정책 으로 구성

### MDP 구성 요소
- 상태(S) = (1, 1) : (1, 1)지점에 존재
- 행동(A) = (1,0,0,0) : 위로 움직임
- 보상함수(R)
  - $R = E[R\|S, A]$
  - (1,1) 지점에서 위로 움직였을 때 받을 보상(E = 기대)
- 상태 변환 확률(P)
  - $P^{ss'}_{a} = P[S = s'\|S = s, A = a]$ : (1, 1) 지점에서 움직일지 말지
- 정책(π)
  - $π(a\|s) = P[A = a\|S = s]$ : (1,1) 위치에서 어디로 이동해야 가장 좋을지


### MDP 외 문제를 풀기위해 필요한 개념
-  감가율( γ )
  -  모든 보상은 감가율이 계산되어 가치로 환산됨
  -  감가율을 고려한 미래보상의 현재 가치(V) = $γ^{𝒌−𝟏} 𝑹_𝒌$


- 반환값 (G)
  -  감가율이 계산된 보삼들의 합
  -  $𝑹_𝟏  + γ𝑹_𝟐 + γ^𝟐 𝑹_𝟑 + … + γ^{𝒏−𝟏} 𝑹_𝒏$


-  가치함수(V)
  -  각각의 노드들이 가지고 있는 가치
  -  이후에 받을 감가율이 계산된 보상들의 합
  -  $V =  E[G \| S = s]$ <br>
  =  $E[𝑹_𝟏  + γ𝑹_𝟐 + γ^𝟐 𝑹_𝟑   + … + γ^{𝒏−𝟏} 𝑹_𝒏]$<br>
  =  $E[𝑹_𝟏 + γV’]$


- Q함수
    -  행동 가치함수 (방향이 정해진 가치함수)
    - 한 상태에서 네 방향으로의 진행이 가능하기 때문에 가치함수를 구하려면 네 방향의 보상과 가치함수를 고려해야 함
    - 오른쪽의 Q함수 = (오른쪽의 보상 + γ * 오른쪽 노드의 가치함수)
    - 오른쪽의 Q함수 = (오른쪽의 보상 + γ * 오른쪽 노드의 다음행동 Q함수)
    - 계산 가능 형태의 가치함수(V)<br>
  	= 각 방향의 정책 * 각 방향의 Q함수 <br>
  	=  $\sum_{a\in A} π(a|s) * q(s, a)$

### 벨만 방정식
- 가치를 계산하는 방정식

### 벨만 기대 방정식
- 정책에 따라서 가치를 판단 ( 정책은 항상 고정되어 있고 정책 발전시에만 변경됨 )
- 가치함수 = 각 방향의 정책 * 각 방향의 (보상 + 감가율 * 다음 상태의 가치함수)
- $𝑽_π (s)$ = $𝑬_π[R + γ𝑽_π (𝑺_{𝒕+𝟏}) \| 𝑺_𝒕 = s]$ <br>
  = $\sum_{𝒂\in 𝑨}π(a|s) * (𝑹_{𝒕+𝟏}+γ𝒗_π (𝒔^′))$

### 벨만 최적 방정식
- 가장 높은 Q함수를 가진 노드만을 선택하여 가치함수 계산 ( 정책과 상관없이 )
- 가치함수 = 보상 + 감가율 * 최고 가치를 가진 방향의 가치함수
- $\prod^{\*}(s,a) =
  \begin{cases}
  1\ if\ a = argmax_{a\in A}q^*(s,a)
  \\\ 0\ otherwise
  \end{cases}$
- $v^*(s)	= maxE[R + γ𝑽_π (𝑺_{𝒕+𝟏}) \| 𝑺_𝒕 = s ]$
- $q^{\*}(s) = E[R + γ max q^* (𝑺_{𝒕+𝟏}, a’) \| 𝑺_𝒕 = s, 𝑨_𝒕=𝒂 ]$
