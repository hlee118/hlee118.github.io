---
layout: post
title: "RL note #5"
date: 2017-12-31
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

#### 목표
- 카트폴 문제를 DQN과 AC로 풀기

#### 카트폴 문제
- 개요
  - OpenAI GYM에 있는 예제
  - 카트 위에 달려있는 폴이 한쪽으로 쓰러지지 않게 카트를 계속해서 움직이는 게임
  - 에이전트 상태 = [x, x', θ, θ'] <br>(x : 카트 위치, x' : 카트 속도, θ : 폴 기운 각도, θ' : 폴 각속도)
- 해결전략 : DQN, AC

#### 딥큐러닝(DQN)
- 개요
  - Q-learning + DNN
  - 인공신경망을 통해 Q함수 근사
  - 딥살사와 동일하나 행동한 곳이 아닌 최대 Q함수로의 update ( 살사와 Q-learning의 차이 )
  - 경험리플레이를 사용
  - 행동 -> 행동 -> 행동 -> ... -> 행동 -> 개선 -> 행동 -> 개선 -> ...

- 경험리플레이
  - 이동경로를 기억해 놨다가 쌓이면 랜덤으로 꺼내서 개선
  - 딥살사는 각각의 위치에서 움직일 action이 정해짐(action 방향으로의 정책 update만 되어야 함 - 온폴리시)
  - Q-learning은 정책update와 action이 따로(최고 q함수로 정책 update을 하면 됨 - 오프폴리시)
  - action과 관련성이 없는 Q-learning에서 경험리플레이의 사용이 가능

- 심층신경망(model, target_model)
  - 현재 상태에 대한 Q함수 예측
  - 입력 : 현재 상태에 대한 정보
  - 심층신경망(입력층1, 은닉층2, 출력층1:linear)
  - 출력 : q함수 테이블

- 행동
  - 현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 만들고 그 중 최고값을 행동으로 선택(a) (가끔식 모험)
  - 행동에 따른 보상(r)과 다음 상태(s')를 추출
  - s, a, r, s' 저장
  - s'으로 이동
  - 일정 이상의 데이터가 쌓일 때까지 계속 행동

- 개선
  - 일정치 이상 데이터가 쌓였다면 개선 시작(이 다음부터는 행동 한 번에 개선 한 번)
  - 데이터들 중 무작위로 추출
  - (s,a)로 q추출(model 사용) / r / (s)로 q'의 최고값 추출(target_model 사용)
  - 딥살사와 다른점은 action과 상관없이 최고 q함수 선택
  - s 입력, (r + 감가율 * q') 정답으로 신경망 개선(model)
  - 한 번의 episode가 끝나면 target_model을 현재 model로 update

DQN에서는 target_model과 model 두 가지의 신경망을 이용한다
{: .notice}

#### 액터-크리틱(AC)
- 개요
  - 폴리시그레이디언트는 반환값(감가율이 계산된 보상값들)을 사용하기에 목표지점까지 도달 후 개선 가능
  - 액터-크리틱은 반환값대신 Q함수를 사용하여 스텝마다 개선이 가능
  - Q함수를 근사하는 인공신경망을 하나 더 만드는 것(=가치신경망)
  - 가치신경망에서 현재의 가치(v)와 다음의 가치(v')를 예측
  - 가치신경망은 cost = ( (R + ᵧv') - v )^2
  - 정책신경망은 기존 반환값 위치를 어드벤티지 함수로 대체
  - 큐함수로 대체하면 변화 정도가 너무 큼 -> 큐함수 - 가치함수(베이스라인) 로 대체 -> 어드벤티지 함수
  - 어드벤티지 함수 = (R + ᵧv') - v
  - 정책신경망 = (log(정책))' * 어드벤티지 함수
  - Actor-Critic(AC) = Advantage Actor-Critic(A2C) 라고도 부름
  - 행동 -> 개선 -> 행동 -> 개선 -> 행동 -> ...


- 신경망1(정책신경망)
  - 현재 상태에 대한 행동확률 예측
  - 입력 : 현재 상태에 대한 정보
  - 출력 : 행동확률 테이블(4)
  - cost를 계산하는 식이 따로 존재(log ...) => keras에서 loss모델 만들어야 됨


- 신경망2(가치신경망)
  - 현재 상태에 대한 가치함수 예측
  - 입력 : 현재 상태에 대한 정보
  - 심층신경망(입력층1, 은닉층2, 출력층1:linear)
  - 출력 : 가치함수(1)
  - 정답이 존재하므로 cost = (정답 - 예측)^2 => keras에서 loss모델 안만들어도 됨


- 행동
  - 현재 상태(s)를 입력으로 정책신경망을 통하여 행동확률 테이블을 만들고 확률적으로 선택하여 이동(룰렛휠)
  - 행동에 따른 보상(r)과 다음 상태(s')를 추출
  - s'으로 이동


- 개선
  - 현재 상태(s)와 다음 상태(s')을 입력으로 가치신경망을 통하여 현재 상태의 가치(v)와 다음 상태의 가치(v')을 예측
  - 정책신경망
    - 어드벤티지 함수 생성 (= (R + ᵧv') - v)
    - 현재 상태(s) 입력, ((log(정책))' * 어드벤티지 함수)가 cost로 신경망 개선
  - 가치신경망
    - target = (R + ᵧv')
    - 현재 상태(s) 입력, target을 정답으로 신경망 개선
