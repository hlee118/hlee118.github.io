---
layout: post
title: "RL note #6"
date: 2017-12-31
description: "..."
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

#### 목표
- 브레이크아웃 게임을 DQN과 A3C로 풀기

#### 브레이크아웃
- 개요
  - 아타리 회사에서 개발한 벽부수기 게임(핑퐁)

#### 브레이크아웃의 문제
- 이미지를 입력형태로 넣기 어려움 -> CNN
- 픽셀 수가 너무 많음 -> 전처리
- 공의 진행방향을 알려면 프레임을 여러개 입력 받아야 함 -> 프레임스킵

#### 브레이크아웃과 CNN(Convolution Neural Net.)
- 개요
  - 영상인식 분야에서 많이 사용되는 신경망
  - 필터를 통한 이미지 추상화 후 입력
  - DQN, A3C 알고리즘으로 행동 개선
{% highlight python %}
  def build_model(self):
      # CNN
      model = Sequential()
      model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu'\
      , input_shape=self.state_size))
      model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu')
      model.add(Conv2D(32, (2, 2), strides=(1,1), activation='relu')
      model.add(Flatten())

      # DQN 신경망
      model.add(Dense(512, activation='relu'))
      model.add(Dense(self.action_size))
      model.summary()
      return model
{% endhighlight %}
#### 전처리
- 이미지를 흑백으로 처리
- 불필요한 공간을 잘라내고 사이즈를 줄임

#### 프레임 스킵
- 여러 프레임을 놓고 일정 간격으로 프레임 선택
- 'BreakoutDeterministic-v4'부터는 자동지원
- 프레임 입력은 [1,2,3,4], [2,3,4,5], ... 식으로 겹쳐서 입력

#### 브레이크아웃과 DQN
- 개요
  - state = 4개의 과거 프레임을 겹친것(depth = 4)
  - 방향 = 왼쪽, 오른쪽, 정지
  - q함수 = 각 방향으로 진행했을 경우 받을 보상의 합(동일)
  - 가장 높은 q함수를 선택하여 정책 개선(가치기반)
  - cost는 기존대로 (정답 Q함수 - 예측 Q함수)^2
  - 경험리플레이를 사용
  - 행동 -> 행동 -> 행동 -> ... -> 행동 -> 개선 -> 행동 -> 개선 -> ...

- 경험리플레이
  - 각각의 상태를 저장해 놨다가 쌓이면 랜덤으로 꺼내서 개선

- 신경망(model, target_model)
  - 현재 상태에 대한 q함수 예측
  - 입력 : 현재 상태에 대한 정보
  - ~~심층신경망(입력층1, 은닉층2, 출력층1:linear)~~
  - 출력 : q함수 테이블


- 행동 및 개선
  - 현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 만들고 그 중 최고값을 행동으로 선택(a) (가끔식 모험)
  - 행동에 따른 보상(r)과 다음 상태(s')를 추출
  - s, a, r, s' 저장
  - s'으로 이동
  - 일정 이상의 데이터가 쌓일 때까지 계속 행동


- 개선
  - 일정치 이상 데이터가 쌓였다면 개선 시작(이 다음부터는 행동 한 번에 개선 한 번)
  - 데이터들 중 무작위로 추출
  - (s,a)로 q추출(model 사용) / r / (s)로 q'의 최고값 추출(target_model 사용)
  - 딥살사와 다른점은 action과 상관없이 최고 q함수 선택
  - s 입력, (r + 감가율  * q') 정답으로 신경망 개선(model)

#### 추가
- 후버로스 그래프
  - 기존 오류함수(2차함수)보다 더 안정적
  - -1 ~ 1 사이의 값은 2차함수를 따르고 나머지 부분은 1차함수 그래프

#### 브레이크아웃과 A3C
