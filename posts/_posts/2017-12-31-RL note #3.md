---
layout: post
title: "RL note #3"
date: 2017-12-31
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

<hr/>
#### 문제점
- 환경에 대한 정보부족 문제
- 계산복잡도가 높다는 문제

<hr/>

#### 목표
- 새로운 모델을 통한 환경에 대한 정보부족 문제 해결

## 그리드월드와 큐러닝
#### 환경에 대한 정보가 없는 문제
- 모든 곳의 보상값을 아는 것 vs 보상값을 모르는 것
- 행동하여 간 곳만 개선(실제 문제)
- 행동 -> 개선 -> 행동 -> 개션 -> ...
- 몬테카를로 예측, 시간차 예측
- SARSA, Q-learning

#### 몬테카를로 예측
- 알파고 작동원리
- 많은 샘플링을 통하여 실제 값의 근사치를 찾아나가는 방법
- 한 번의 경로가 하나의 샘플링이 되어 그 경로의 값들을 개선


#### 시간차 예측
- 몬테카를로는 실시간이 아님
- 한 번의 스텝으로도 가치함수를 update하는 방법

#### SARSA
- S = state, A = action, R = reward, S = state, A = action
- 시간차 예측을 통한 학습 알고리즘
- 정책평가 + 정책발전 = 개선(Q함수 update)
- 행동 -> 개선 -> 행동 -> …
- 행동(입실론 탐욕)
  - 행동은 기본적으로 가장 높은 Q함수로 진행
  - 입실론의 확률로 랜덤 노드를 선택
- 개선
  - 행동한 방향으로 Q함수 update
- 입실론 탐욕정책으로 행동하고 그 방향으로 발전(온폴리시)

원래는 Q = 보상 + 감가율 \* 가치함수<br>
탐욕정책으로 인하여 가장 높은 Q함수 = 가치함수<br>
Q = 다음 노드의 보상 + 감가율 * Q'
{: .notice}

#### Q-learning
- 행동
  - SARSA와 동일하게 탐욕정책
- 개선
  - SARSA와 동일하지만 만약 에이전트가 모험을 선택했다가 더 안좋은 상황이 생겼다면?
  - 행동한 곳 기준으로 개선하는 것이 아니라 무조건 가장높은 Q'값 기준으로 개선
  - SARSA의 문제점 보완

`환경문제 해결`

<hr/>
#### 남은 문제점
- 계산복잡도가 높다는 문제

<hr/>
