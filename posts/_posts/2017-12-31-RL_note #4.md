---
layout: post
title: "RL note #4"
date: 2017-12-31
excerpt: "Machine Learning"
tags:
- Reinforcement Learning
ref:
- 이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'
comments: true
---

# 강화학습

<hr/>
#### 남은 문제점
- 계산복잡도가 높다는 문제

<hr/>

#### 목표
- NN을 이용한 계산복잡도 문제 해결

## 그리드월드와 근사함수
#### 계산복잡도가 높다는 문제
- 차원의 수, 노드의 수가 많아지면 Q함수를 가지고 있기 너무 큼
- 10 * 10 그리드월드라 할 때 행동이 4가지이기 때문에 10 * 10 * 4의 Q함수 테이블을 연산에 이용해야 함
- 근사함수를 통한 가치함수의 매개변수화를 통해 해결가능
- SARSA -> 딥살사
- 몬테카를로 -> 폴리시 그레이디언트
- Q-learning -> DQN(Deep Q-network)

#### 인공신경망(NN)과 케라스
- 인공신경망에 대한 설명은 생락
- 케라스에서 인공신경망 구현 코드
{% highlight python %}
  from keras.layers import Dense
  from keras.models import Sequential

  x_train = []
  y_train = []

  model = Sequential()
  model.add(Dense(12, input_dim=5, activation='sigmoid'))
  model.add(Dense(30, activation='sigmoid'))
  model.add(Dense(1, activation='RMSProp'))
  model.compile(loss='mse', optimizer='RMSProp')

  model.fit(x_train, y_train, batch_size=32, epoch=1)
{% endhighlight %}

#### 딥살사 이론
- 개요
  - 살사 + DNN
  - 살사이론의 q함수 테이블을 근사함수를 만들어 저장
  - 행동시 지정 위치의 q함수들만 예측
  - 개선은 다음 지점의 q함수를 정답으로 하여 인공신경망 개선교육

- 심층신경망
  - 현재 상태에 대한 Q함수 예측
  - 입력 : 현재 상태에 대한 정보
  - 심층신경망(입력층1, 은닉층2, 출력층1:linear)
  - 출력 : q함수 테이블

- 행동
  - 현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 생성
  - q함수 테이블에서 최고값으로 행동으로 선택(a)

- 개선
  - 심층신경망의 가중치를 개선 ( = Q함수 개선 )
  - 살사에서의 오류값을 이용
  - 오차 = (정답 – 예측)
  - 오차 = R + γQ(S2, A2) – Q(S1, S2)

정책기반 강화학습 vs 가치기반 강화학습<br>
정책기반 : 정책에 따라 행동(정책평가가 이루어지지 않으면 최고가치가 존재해도 따라가지 않음)<br>
가치기반 : 최고가치가 최고정책으라는 가정하에 행동
{: .notice}


#### 폴리시 그레이디언트
- 개요
  - 몬테카를로 + DNN
  - 행동 -> 행동 -> 행동 -> ... -> 개선
  - q함수 근사가 아닌 정책 근사로 행동 및 개선
  - 탐욕정책이 아닌 각 방향으로 갈 확률 근사


- 심층신경망
  - 현재 상태에 대한 각 방향의 정책 예측
  - 입력 : 현재 상태에 대한 정보
  - 심층신경망(입력층1, 은닉층2, 출력층1:sofrmax)
  - 출력 : 각 행동을 할 확률


- 행동
  - 현재 상태(s)를 입력으로 심층신경망을 통하여 각 방향으로 갈 확률 계산
  - 확률 기반으로 하나 선택(룰렛휠)
  - 끝날 때까지 계속 이동


- 개선
  - 지나온 경로 각각의 cost 계산
  - s 입력, cost로 개선

`계산복잡도 문제 해결`
