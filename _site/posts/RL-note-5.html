<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]-->
<head>
        <meta charset="UTF-8">
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
    <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1">
    <title>RL note #5 &#8211; </title>
    <meta name="description" content="blog">
    <meta name="keywords" content="Reinforcement Learning">
    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="/assets/img/logo.png">
    <meta name="twitter:title" content="RL note #5">
    <meta name="twitter:description" content="Machine Learning">
    
    <!-- Open Graph -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="RL note #5">
    <meta property="og:description" content="Machine Learning">
    <meta property="og:url" content="/posts/RL-note-5">
    <meta property="og:site_name" content="">
    <meta property="og:image" content="/assets/img/logo.png">
    
    
    
    <link rel="canonical" href="/posts/RL-note-5">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">
    <!-- Handheld -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <!-- JS -->
    <script src="/assets/js/modernizr-3.3.1.custom.min.js"></script>
    <!-- Favicons -->
    <link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
    <link rel="shortcut icon" type="image/png" href="/favicon.png" />
    <link rel="shortcut icon" href="/favicon.ico" />
    <!-- Background Image -->
    
    
    
    <style type="text/css">body {background-image:url(/assets/img/placeholder-big.jpg);  background-repeat: no-repeat; background-size: cover; }</style>
    
    <!-- Post Feature Image -->
    

    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    equationNumbers: {
      autoNumber: "AMS"
    }
},
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
  alert("Math Processing Error: "+message[1]);
});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
  alert("Math Processing Error: "+message[1]);
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</head>
<body>
    	<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
		<button class="dl-trigger">Open Menu</button>
		<ul class="dl-menu">
			<li><a href="/">Home</a></li>
			<li><a href="/about/en">About</a></li>
			<li><a href="/projects">Projects</a></li>
			<li><a href="/posts/">Posts</a></li>
		</ul><!-- /.dl-menu -->
	</nav><!-- /.dl-menuwrapper -->

    <!-- Header -->
    <header class="header" role="banner">
        <div class="wrapper animated fadeIn">
            <button class="btn zoombtn" onclick="history.back()"><i class="fa fa-chevron-left"></i></button>
            <div class="content">
                <div class="page-title ">
                    <h1>RL note #5</h1>
                    <h4>31 Dec 2017</h4>
                    
                </div>
                <h1 id="강화학습">강화학습</h1>

<h4 id="목표">목표</h4>
<ul>
  <li>카트폴 문제를 DQN과 AC로 풀기</li>
</ul>

<h4 id="카트폴-문제">카트폴 문제</h4>
<ul>
  <li>개요
    <ul>
      <li>OpenAI GYM에 있는 예제</li>
      <li>카트 위에 달려있는 폴이 한쪽으로 쓰러지지 않게 카트를 계속해서 움직이는 게임</li>
      <li>에이전트 상태 = [x, x’, θ, θ’] <br />(x : 카트 위치, x’ : 카트 속도, θ : 폴 기운 각도, θ’ : 폴 각속도)</li>
    </ul>
  </li>
  <li>해결전략 : DQN, AC</li>
</ul>

<h4 id="딥큐러닝dqn">딥큐러닝(DQN)</h4>
<ul>
  <li>개요
    <ul>
      <li>Q-learning + DNN</li>
      <li>인공신경망을 통해 Q함수 근사</li>
      <li>딥살사와 동일하나 행동한 곳이 아닌 최대 Q함수로의 update ( 살사와 Q-learning의 차이 )</li>
      <li>경험리플레이를 사용</li>
      <li>행동 -&gt; 행동 -&gt; 행동 -&gt; … -&gt; 행동 -&gt; 개선 -&gt; 행동 -&gt; 개선 -&gt; …</li>
    </ul>
  </li>
  <li>경험리플레이
    <ul>
      <li>이동경로를 기억해 놨다가 쌓이면 랜덤으로 꺼내서 개선</li>
      <li>딥살사는 각각의 위치에서 움직일 action이 정해짐(action 방향으로의 정책 update만 되어야 함 - 온폴리시)</li>
      <li>Q-learning은 정책update와 action이 따로(최고 q함수로 정책 update을 하면 됨 - 오프폴리시)</li>
      <li>action과 관련성이 없는 Q-learning에서 경험리플레이의 사용이 가능</li>
    </ul>
  </li>
</ul>

<p class="notice">만약 딥살사에서 경험리플레이를 사용한다면?<br />
경로의 마지막부터 거꾸로 학습이 된다고 할 때 잘못된 모험을 하여 q함수값이 감소한다면<br />
그 경로 위의 모든 노드는 q함수값이 감소하게 될 것!</p>

<ul>
  <li>심층신경망(model, target_model)
    <ul>
      <li>현재 상태에 대한 Q함수 예측</li>
      <li>입력 : 현재 상태에 대한 정보</li>
      <li>심층신경망(입력층1, 은닉층2, 출력층1:linear)</li>
      <li>출력 : q함수 테이블</li>
    </ul>
  </li>
  <li>행동
    <ul>
      <li>현재 상태(s)를 입력으로 심층신경망을 통하여 q함수 테이블을 만들고 그 중 최고값을 행동으로 선택(a) (가끔식 모험)</li>
      <li>행동에 따른 보상(r)과 다음 상태(s’)를 추출</li>
      <li>s, a, r, s’ 저장</li>
      <li>s’으로 이동</li>
      <li>일정 이상의 데이터가 쌓일 때까지 계속 행동</li>
    </ul>
  </li>
  <li>개선
    <ul>
      <li>일정치 이상 데이터가 쌓였다면 개선 시작(이 다음부터는 행동 한 번에 개선 한 번)</li>
      <li>데이터들 중 무작위로 추출</li>
      <li>(s,a)로 q추출(model 사용) / r / (s)로 q’의 최고값 추출(target_model 사용)</li>
      <li>딥살사와 다른점은 action과 상관없이 최고 q함수 선택</li>
      <li>s 입력, (r + 감가율 * q’) 정답으로 신경망 개선(model)</li>
      <li>한 번의 episode가 끝나면 target_model을 현재 model로 update</li>
    </ul>
  </li>
</ul>

<p class="notice">DQN에서는 target_model과 model 두 가지의 신경망을 이용한다</p>

<h4 id="액터-크리틱ac">액터-크리틱(AC)</h4>
<ul>
  <li>개요
    <ul>
      <li>폴리시그레이디언트는 반환값(감가율이 계산된 보상값들)을 사용하기에 목표지점까지 도달 후 개선 가능</li>
      <li>액터-크리틱은 반환값대신 Q함수를 사용하여 스텝마다 개선이 가능</li>
      <li>Q함수를 근사하는 인공신경망을 하나 더 만드는 것(=가치신경망)</li>
      <li>가치신경망에서 현재의 가치(v)와 다음의 가치(v’)를 예측</li>
      <li>가치신경망은 cost = ( (R + ᵧv’) - v )^2</li>
      <li>정책신경망은 기존 반환값 위치를 어드벤티지 함수로 대체</li>
      <li>큐함수로 대체하면 변화 정도가 너무 큼 -&gt; 큐함수 - 가치함수(베이스라인) 로 대체 -&gt; 어드벤티지 함수</li>
      <li>어드벤티지 함수 = (R + ᵧv’) - v</li>
      <li>정책신경망 = (log(정책))’ * 어드벤티지 함수</li>
      <li>Actor-Critic(AC) = Advantage Actor-Critic(A2C) 라고도 부름</li>
      <li>행동 -&gt; 개선 -&gt; 행동 -&gt; 개선 -&gt; 행동 -&gt; …</li>
    </ul>
  </li>
  <li>신경망1(정책신경망)
    <ul>
      <li>현재 상태에 대한 행동확률 예측</li>
      <li>입력 : 현재 상태에 대한 정보</li>
      <li>출력 : 행동확률 테이블(4)</li>
      <li>cost를 계산하는 식이 따로 존재(log …) =&gt; keras에서 loss모델 만들어야 됨</li>
    </ul>
  </li>
  <li>신경망2(가치신경망)
    <ul>
      <li>현재 상태에 대한 가치함수 예측</li>
      <li>입력 : 현재 상태에 대한 정보</li>
      <li>심층신경망(입력층1, 은닉층2, 출력층1:linear)</li>
      <li>출력 : 가치함수(1)</li>
      <li>정답이 존재하므로 cost = (정답 - 예측)^2 =&gt; keras에서 loss모델 안만들어도 됨</li>
    </ul>
  </li>
  <li>행동
    <ul>
      <li>현재 상태(s)를 입력으로 정책신경망을 통하여 행동확률 테이블을 만들고 확률적으로 선택하여 이동(룰렛휠)</li>
      <li>행동에 따른 보상(r)과 다음 상태(s’)를 추출</li>
      <li>s’으로 이동</li>
    </ul>
  </li>
  <li>개선
    <ul>
      <li>현재 상태(s)와 다음 상태(s’)을 입력으로 가치신경망을 통하여 현재 상태의 가치(v)와 다음 상태의 가치(v’)을 예측</li>
      <li>정책신경망
        <ul>
          <li>어드벤티지 함수 생성 (= (R + ᵧv’) - v)</li>
          <li>현재 상태(s) 입력, ((log(정책))’ * 어드벤티지 함수)가 cost로 신경망 개선</li>
        </ul>
      </li>
      <li>가치신경망
        <ul>
          <li>target = (R + ᵧv’)</li>
          <li>현재 상태(s) 입력, target을 정답으로 신경망 개선</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

                <div class="entry-meta">
                <br>
<hr>

<div style="width:100%">
<div style="width:10%;float:left;">
    <span style="color:#999999;">출처</span>
</div>
<div style="width:90%;float:left;">

    <span style="color:#999999;">이웅원 외 4명 저 '파이썬과 케라스로 배우는 강화학습'</span><br>

</div>
<span style="color:black;"></span>
<span class="refer-tags">
</span>
</div>

<span class="entry-tags"><a href="/tags/#Reinforcement Learning" title="Pages tagged Reinforcement Learning" class="tag"><span class="term">Reinforcement Learning</span></a></span>
<div style="clear:both"></div>

                </div>
            </div>
        </div>
        <!-- <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> -->
    </header>
        <!-- JS -->
    <script src="/assets/js/jquery-1.12.0.min.js"></script>
    <script src="/assets/js/jquery.dlmenu.min.js"></script>
    <script src="/assets/js/jquery.goup.min.js"></script>
    <script src="/assets/js/jquery.magnific-popup.min.js"></script>
    <script src="/assets/js/jquery.fitvid.min.js"></script>
    <script src="/assets/js/scripts.js"></script>
    
    
    <!-- MathJax -->
    <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
</body>
</html>
